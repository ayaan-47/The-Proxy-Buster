{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17fa7221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 339ms/step\n",
      "1/1 [==============================] - 0s 208ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "14/14 [==============================] - 0s 8ms/step\n",
      "2/2 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from keras_facenet import FaceNet\n",
    "from mtcnn import MTCNN\n",
    "import joblib\n",
    "# Path to the dataset folder\n",
    "dataset_folder = \"data\"\n",
    "\n",
    "# Initialize arrays to store face embeddings and labels\n",
    "face_embeddings = []\n",
    "labels = []\n",
    "\n",
    "# Load the FaceNet model\n",
    "model = FaceNet()\n",
    "\n",
    "# Iterate over subfolders in the dataset folder\n",
    "for subfolder_name in os.listdir(dataset_folder):\n",
    "    subfolder_path = os.path.join(dataset_folder, subfolder_name)\n",
    "    if not os.path.isdir(subfolder_path):\n",
    "        continue\n",
    "\n",
    "    # Load images from the subfolder\n",
    "    for image_name in os.listdir(subfolder_path):\n",
    "        image_path = os.path.join(subfolder_path, image_name)\n",
    "        if not os.path.isfile(image_path):\n",
    "            continue\n",
    "\n",
    "        # Load and preprocess the image\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (160, 160))  # Resize image to match FaceNet input size\n",
    "\n",
    "        # Extract the face embedding\n",
    "        face_embedding = model.embeddings(np.expand_dims(image, axis=0))[0]\n",
    "\n",
    "        # Add the face embedding and corresponding label to the arrays\n",
    "        face_embeddings.append(face_embedding)\n",
    "        labels.append(subfolder_name)\n",
    "\n",
    "# Convert labels to numerical representation\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Train a support vector machine (SVM) classifier\n",
    "classifier = SVC(kernel='linear')\n",
    "classifier.fit(face_embeddings, labels)\n",
    "\n",
    "# Save the SVM model\n",
    "svm_model_path = \"svm_model.pkl\"\n",
    "joblib.dump(classifier, svm_model_path)\n",
    "\n",
    "# Test the trained model on a new image\n",
    "test_image_path = \"group2.jpg\"\n",
    "test_image = cv2.imread(test_image_path)\n",
    "test_image = cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB)\n",
    "test_image_resized = cv2.resize(test_image, (160, 160))  # Resize image to match FaceNet input size\n",
    "\n",
    "# Detect faces in the test image using MTCNN\n",
    "detector = MTCNN()\n",
    "faces = detector.detect_faces(test_image)\n",
    "\n",
    "# Iterate over detected faces\n",
    "for face in faces:\n",
    "    # Extract the bounding box coordinates\n",
    "    x, y, w, h = face['box']\n",
    "    \n",
    "    # Extract the face region\n",
    "    face_image = test_image[y:y+h, x:x+w]\n",
    "    face_image_resized = cv2.resize(face_image, (160, 160))  # Resize face region to match FaceNet input size\n",
    "\n",
    "    # Extract the face embedding for the face region\n",
    "    test_face_embedding = model.embeddings(np.expand_dims(face_image_resized, axis=0))[0]\n",
    "\n",
    "    # Predict the label for the face region\n",
    "    predicted_label = classifier.predict([test_face_embedding])\n",
    "\n",
    "    # Convert the predicted label back to the original class name\n",
    "    predicted_person = label_encoder.inverse_transform(predicted_label)[0]\n",
    "\n",
    "    # Draw rectangle around the face\n",
    "    color = (0, 255, 0) if predicted_person != \"unknown\" else (0, 0, 255)\n",
    "    cv2.rectangle(test_image, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "    # Put text as the name of the person recognized or unknown\n",
    "    text = predicted_person if predicted_person != \"unknown\" else \"Unknown\"\n",
    "    cv2.putText(test_image, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "# Display the test image\n",
    "cv2.imshow(\"Test Image\", test_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f82d2d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 296ms/step\n",
      "1/1 [==============================] - 0s 203ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "14/14 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from keras_facenet import FaceNet\n",
    "from mtcnn import MTCNN\n",
    "import joblib\n",
    "\n",
    "# Path to the dataset folder\n",
    "dataset_folder = \"data\"\n",
    "\n",
    "# Initialize arrays to store face embeddings and labels\n",
    "face_embeddings = []\n",
    "labels = []\n",
    "\n",
    "# Load the FaceNet model\n",
    "model = FaceNet()\n",
    "\n",
    "# Iterate over subfolders in the dataset folder\n",
    "for subfolder_name in os.listdir(dataset_folder):\n",
    "    subfolder_path = os.path.join(dataset_folder, subfolder_name)\n",
    "    if not os.path.isdir(subfolder_path):\n",
    "        continue\n",
    "\n",
    "    # Load images from the subfolder\n",
    "    for image_name in os.listdir(subfolder_path):\n",
    "        image_path = os.path.join(subfolder_path, image_name)\n",
    "        if not os.path.isfile(image_path):\n",
    "            continue\n",
    "\n",
    "        # Load and preprocess the image\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (160, 160))  # Resize image to match FaceNet input size\n",
    "\n",
    "        # Extract the face embedding\n",
    "        face_embedding = model.embeddings(np.expand_dims(image, axis=0))[0]\n",
    "\n",
    "        # Add the face embedding and corresponding label to the arrays\n",
    "        face_embeddings.append(face_embedding)\n",
    "        labels.append(subfolder_name)\n",
    "\n",
    "# Convert labels to numerical representation\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Train a support vector machine (SVM) classifier\n",
    "classifier = SVC(kernel='linear')\n",
    "classifier.fit(face_embeddings, labels)\n",
    "\n",
    "# Save the SVM model\n",
    "svm_model_path = \"svm_model.pkl\"\n",
    "joblib.dump(classifier, svm_model_path)\n",
    "\n",
    "# Test the trained model on a new image\n",
    "test_image_path = \"group2.jpg\"\n",
    "test_image = cv2.imread(test_image_path)\n",
    "test_image = cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB)\n",
    "test_image_resized = cv2.resize(test_image, (160, 160))  # Resize image to match FaceNet input size\n",
    "\n",
    "# Detect faces in the test image using MTCNN\n",
    "detector = MTCNN()\n",
    "faces = detector.detect_faces(test_image)\n",
    "\n",
    "# Iterate over detected faces\n",
    "for face in faces:\n",
    "    # Extract the bounding box coordinates\n",
    "    x, y, w, h = face['box']\n",
    "    \n",
    "    # Extract the face region\n",
    "    face_image = test_image[y:y+h, x:x+w]\n",
    "    face_image_resized = cv2.resize(face_image, (160, 160))  # Resize face region to match FaceNet input size\n",
    "\n",
    "    # Extract the face embedding for the face region\n",
    "    test_face_embedding = model.embeddings(np.expand_dims(face_image_resized, axis=0))[0]\n",
    "\n",
    "    # Predict the label for the face region\n",
    "    predicted_label = classifier.predict([test_face_embedding])\n",
    "\n",
    "    # Get the confidence scores for all classes\n",
    "    confidence_scores = classifier.decision_function([test_face_embedding])\n",
    "    max_confidence_index = np.argmax(confidence_scores)\n",
    "    max_confidence_score = confidence_scores[0][max_confidence_index]\n",
    "    \n",
    "    # Convert the predicted label back to the original class name\n",
    "    predicted_person = label_encoder.inverse_transform(predicted_label)[0]\n",
    "\n",
    "    # Draw rectangle around the face\n",
    "    color = (0, 255, 0) if predicted_person != \"unknown\" else (0, 0, 255)\n",
    "    cv2.rectangle(test_image, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "    # Put text as the name of the person recognized or unknown with confidence percentage\n",
    "    if predicted_person != \"unknown\":\n",
    "        text = f\"{predicted_person} ({int(max_confidence_score * 100)}% confidence)\"\n",
    "    else:\n",
    "        text = \"Unknown\"\n",
    "    cv2.putText(test_image, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "# Display the test image\n",
    "cv2.imshow(\"Test Image\", test_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60c991e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 410ms/step\n",
      "1/1 [==============================] - 0s 226ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "7/7 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from keras_facenet import FaceNet\n",
    "from mtcnn import MTCNN\n",
    "import joblib\n",
    "\n",
    "# Path to the dataset folder\n",
    "dataset_folder = \"data\"\n",
    "\n",
    "# Initialize arrays to store face embeddings and labels\n",
    "face_embeddings = []\n",
    "labels = []\n",
    "\n",
    "# Load the FaceNet model\n",
    "model = FaceNet()\n",
    "\n",
    "# Iterate over subfolders in the dataset folder\n",
    "for subfolder_name in os.listdir(dataset_folder):\n",
    "    subfolder_path = os.path.join(dataset_folder, subfolder_name)\n",
    "    if not os.path.isdir(subfolder_path):\n",
    "        continue\n",
    "\n",
    "    # Load images from the subfolder\n",
    "    for image_name in os.listdir(subfolder_path):\n",
    "        image_path = os.path.join(subfolder_path, image_name)\n",
    "        if not os.path.isfile(image_path):\n",
    "            continue\n",
    "\n",
    "        # Load and preprocess the image\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (160, 160))  # Resize image to match FaceNet input size\n",
    "\n",
    "        # Extract the face embedding\n",
    "        face_embedding = model.embeddings(np.expand_dims(image, axis=0))[0]\n",
    "\n",
    "        # Add the face embedding and corresponding label to the arrays\n",
    "        face_embeddings.append(face_embedding)\n",
    "        labels.append(subfolder_name)\n",
    "\n",
    "# Convert labels to numerical representation\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Train a support vector machine (SVM) classifier\n",
    "classifier = SVC(kernel='linear')\n",
    "classifier.fit(face_embeddings, labels)\n",
    "\n",
    "# Save the SVM model\n",
    "svm_model_path = \"svm_model.pkl\"\n",
    "joblib.dump(classifier, svm_model_path)\n",
    "\n",
    "# Test the trained model on a new image\n",
    "test_image_path = \"group4.jpg\"\n",
    "test_image = cv2.imread(test_image_path)\n",
    "test_image = cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB)\n",
    "test_image_resized = cv2.resize(test_image, (160, 160))  # Resize image to match FaceNet input size\n",
    "\n",
    "# Detect faces in the test image using MTCNN\n",
    "detector = MTCNN()\n",
    "faces = detector.detect_faces(test_image)\n",
    "\n",
    "# Iterate over detected faces\n",
    "for face in faces:\n",
    "    # Extract the bounding box coordinates\n",
    "    x, y, w, h = face['box']\n",
    "    \n",
    "    # Extract the face region\n",
    "    face_image = test_image[y:y+h, x:x+w]\n",
    "    face_image_resized = cv2.resize(face_image, (160, 160))  # Resize face region to match FaceNet input size\n",
    "\n",
    "    # Extract the face embedding for the face region\n",
    "    test_face_embedding = model.embeddings(np.expand_dims(face_image_resized, axis=0))[0]\n",
    "\n",
    "    # Predict the label for the face region\n",
    "    predicted_label = classifier.predict([test_face_embedding])\n",
    "\n",
    "    # Get the confidence scores for all classes\n",
    "    confidence_scores = classifier.decision_function([test_face_embedding])\n",
    "    max_confidence_index = np.argmax(confidence_scores)\n",
    "    max_confidence_score = confidence_scores[0][max_confidence_index]\n",
    "    \n",
    "    # Convert the predicted label back to the original class name\n",
    "    predicted_person = label_encoder.inverse_transform(predicted_label)[0]\n",
    "\n",
    "    # Determine the confidence threshold\n",
    "    confidence_threshold = 0.5\n",
    "\n",
    "    # Draw rectangle around the face\n",
    "    if max_confidence_score >= confidence_threshold:\n",
    "        text = f\"{predicted_person} ({int(max_confidence_score * 100)}% confidence)\"\n",
    "        color = (0, 255, 0)\n",
    "    else:\n",
    "        text = \"Unknown\"\n",
    "        color = (0, 0, 255)\n",
    "\n",
    "    cv2.rectangle(test_image, (x, y), (x + w, y + h), color, 2)\n",
    "    cv2.putText(test_image, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "# Display the test image\n",
    "cv2.imshow(\"Test Image\", test_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7270ac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from keras_facenet import FaceNet\n",
    "from mtcnn import MTCNN\n",
    "import joblib\n",
    "\n",
    "# Path to the dataset folder\n",
    "dataset_folder = \"data\"\n",
    "\n",
    "# Load the FaceNet model\n",
    "model = FaceNet()\n",
    "\n",
    "# Check if the SVM model is already saved\n",
    "svm_model_path = \"svm_model.pkl\"\n",
    "if os.path.exists(svm_model_path):\n",
    "    # Load the SVM model\n",
    "    classifier = joblib.load(svm_model_path)\n",
    "    label_encoder = joblib.load(\"label_encoder.pkl\")\n",
    "else:\n",
    "    # Initialize arrays to store face embeddings and labels\n",
    "    face_embeddings = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate over subfolders in the dataset folder\n",
    "    for subfolder_name in os.listdir(dataset_folder):\n",
    "        subfolder_path = os.path.join(dataset_folder, subfolder_name)\n",
    "        if not os.path.isdir(subfolder_path):\n",
    "            continue\n",
    "\n",
    "        # Load images from the subfolder\n",
    "        for image_name in os.listdir(subfolder_path):\n",
    "            image_path = os.path.join(subfolder_path, image_name)\n",
    "            if not os.path.isfile(image_path):\n",
    "                continue\n",
    "\n",
    "            # Load and preprocess the image\n",
    "            image = cv2.imread(image_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = cv2.resize(image, (160, 160))  # Resize image to match FaceNet input size\n",
    "\n",
    "            # Extract the face embedding\n",
    "            face_embedding = model.embeddings(np.expand_dims(image, axis=0))[0]\n",
    "\n",
    "            # Add the face embedding and corresponding label to the arrays\n",
    "            face_embeddings.append(face_embedding)\n",
    "            labels.append(subfolder_name)\n",
    "\n",
    "    # Convert labels to numerical representation\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "    # Train a support vector machine (SVM) classifier\n",
    "    classifier = SVC(kernel='linear')\n",
    "    classifier.fit(face_embeddings, labels)\n",
    "\n",
    "    # Save the SVM model and label encoder\n",
    "    joblib.dump(classifier, svm_model_path)\n",
    "    joblib.dump(label_encoder, \"label_encoder.pkl\")\n",
    "\n",
    "# Test the trained model on a new image\n",
    "test_image_path = \"iamges.jpg\"\n",
    "test_img = cv2.imread(test_image_path)\n",
    "test_image = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
    "test_image_resized = cv2.resize(test_image, (160, 160))\n",
    "test_image_resized = cv2.resize(test_img, (160, 160))# Resize image to match FaceNet input size\n",
    "\n",
    "# Detect faces in the test image using MTCNN\n",
    "detector = MTCNN()\n",
    "faces = detector.detect_faces(test_image)\n",
    "\n",
    "# Iterate over detected faces\n",
    "for face in faces:\n",
    "    # Extract the bounding box coordinates\n",
    "    x, y, w, h = face['box']\n",
    "    \n",
    "    # Extract the face region\n",
    "    face_image = test_image[y:y+h, x:x+w]\n",
    "    face_image_resized = cv2.resize(face_image, (160, 160))  # Resize face region to match FaceNet input size\n",
    "\n",
    "    # Extract the face embedding for the face region\n",
    "    test_face_embedding = model.embeddings(np.expand_dims(face_image_resized, axis=0))[0]\n",
    "\n",
    "    # Predict the label for the face region\n",
    "    predicted_label = classifier.predict([test_face_embedding])\n",
    "\n",
    "    # Get the confidence scores for all classes\n",
    "    confidence_scores = classifier.decision_function([test_face_embedding])\n",
    "    max_confidence_index = np.argmax(confidence_scores)\n",
    "    max_confidence_score = confidence_scores[0][max_confidence_index]\n",
    "    \n",
    "    # Convert the predicted label back to the original class name\n",
    "    predicted_person = label_encoder.inverse_transform(predicted_label)[0]\n",
    "\n",
    "    # Determine the confidence threshold\n",
    "    confidence_threshold = 0.5\n",
    "\n",
    "    # Draw rectangle around the face\n",
    "    if max_confidence_score >= confidence_threshold:\n",
    "        text = f\"{predicted_person} ({int(max_confidence_score * 100)}% confidence)\"\n",
    "        color = (0, 255, 0)\n",
    "    else:\n",
    "        text = \"Unknown\"\n",
    "        color = (0, 0, 255)\n",
    "\n",
    "    cv2.rectangle(test_img, (x, y), (x + w, y + h), color, 2)\n",
    "    cv2.putText(test_img, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "# Display the test image\n",
    "cv2.imshow(\"Test Image\", test_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67fad46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
